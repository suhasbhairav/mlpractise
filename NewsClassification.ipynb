{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXVyl2xfdgDml/GC1dLvhE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLTEnJ8SHns7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztbPnSpnH8ta"
      },
      "source": [
        "df = pd.read_json(\"news.json\", lines=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lgxy2lgMQ6K"
      },
      "source": [
        "import re\n",
        "def tokenize_url(url:str):\n",
        "  url = url.replace(\"https://www.huffingtonpost.com/entry/\", \"\")\n",
        "  url=re.sub(\"(\\W|_)+\", \" \", url)\n",
        "  return url\n",
        "\n",
        "df[\"tokenized_url\"] = df['link'].apply(lambda x:tokenize_url(x))\n",
        "\n",
        "df[\"text_desc\"] = df[\"short_description\"]\n",
        "\n",
        "df[\"tex_desc_headline\"] = df[\"short_description\"] + \" \" + df[\"headline\"]\n",
        "\n",
        "df[\"text_desc_headline_url\"] = df[\"short_description\"] + \" \" + df[\"headline\"] + \" \" + df[\"tokenized_url\"]\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDdEpX-UaKD"
      },
      "source": [
        "def _reciprocal_rank(true_labels: list, machine_preds: list):\n",
        "    \"\"\"Compute the reciprocal rank at cutoff k\"\"\"\n",
        "    \n",
        "    # add index to list only if machine predicted label exists in true labels\n",
        "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_preds) if r in true_labels]\n",
        "\n",
        "    rr = 0\n",
        "    if len(tp_pos_list) > 0:\n",
        "        # for RR we need position of first correct item\n",
        "        first_pos_list = tp_pos_list[0]\n",
        "        \n",
        "        # rr = 1/rank\n",
        "        rr = 1 / float(first_pos_list)\n",
        "\n",
        "    return rr\n",
        "\n",
        "def compute_mrr_at_k(items:list):\n",
        "    \"\"\"Compute the MRR (average RR) at cutoff k\"\"\"\n",
        "    rr_total = 0\n",
        "    \n",
        "    for item in items:   \n",
        "        rr_at_k = _reciprocal_rank(item[0],item[1])\n",
        "        rr_total = rr_total + rr_at_k\n",
        "        mrr = rr_total / 1/float(len(items))\n",
        "\n",
        "    return mrr\n",
        "\n",
        "def collect_preds(Y_test,Y_preds):\n",
        "    \"\"\"Collect all predictions and ground truth\"\"\"\n",
        "    \n",
        "    pred_gold_list=[[[Y_test[idx]],pred] for idx,pred in enumerate(Y_preds)]\n",
        "    return pred_gold_list\n",
        "             \n",
        "def compute_accuracy(eval_items:list):\n",
        "    correct=0\n",
        "    total=0\n",
        "    \n",
        "    for item in eval_items:\n",
        "        true_pred=item[0]\n",
        "        machine_pred=set(item[1])\n",
        "        \n",
        "        for cat in true_pred:\n",
        "            if cat in machine_pred:\n",
        "                correct+=1\n",
        "                break\n",
        "    \n",
        "    \n",
        "    accuracy=correct/float(len(eval_items))\n",
        "    return accuracy"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig4weyr4UbRY"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLSIJl1oUvrI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG_6tA05Uy1g"
      },
      "source": [
        "def extract_features(df, field, training_data, testing_data, type=\"binary\"):\n",
        "  if \"binary\" in type:\n",
        "    cv = CountVectorizer(binary=True, max_df=0.95)\n",
        "    cv.fit_transform(training_data[field].values)\n",
        "\n",
        "    train_feature_set = cv.transform(training_data[field].values)\n",
        "    test_feature_set = cv.transform(testing_data[field].values)\n",
        "\n",
        "    return train_feature_set, test_feature_set, cv\n",
        "\n",
        "  elif \"counts\" in type:\n",
        "    cv = CountVectorizer(binary=False, max_df=0.95)\n",
        "    cv.fit_transform(training_data[field].values)\n",
        "\n",
        "    train_feature_set = cv.transform(training_data[field].values)\n",
        "    test_feature_set = cv.transform(testing_data[field].values)\n",
        "\n",
        "    return train_feature_set, test_feature_set, cv\n",
        "\n",
        "  else:\n",
        "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.95)\n",
        "    tfidf_vectorizer.fit_transform(training_data[field].values)\n",
        "\n",
        "    train_feature_set = tfidf_vectorizer.transform(training_data[field].values)\n",
        "    test_feature_set = tfidf_vectorizer.transform(testing_data[field].values)\n",
        "\n",
        "    return train_feature_set, test_feature_set, cv"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbQM4DAcWfsD"
      },
      "source": [
        "def get_top_k_predictions(model, X_test, k):\n",
        "  probs = model.predict_proba(X_test)\n",
        "\n",
        "  # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
        "  best_n = np.argsort(probs, axis=1)[:,-k:]\n",
        "  \n",
        "  # GET CATEGORY OF PREDICTIONS\n",
        "  preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
        "  \n",
        "  preds=[ item[::-1] for item in preds]\n",
        "  \n",
        "  return preds"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDpgeNYrXBEG"
      },
      "source": [
        "def train_model(df,field=\"text_desc\",feature_rep=\"binary\",top_k=3):\n",
        "    \n",
        "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
        "    training_data, testing_data = train_test_split(df,random_state = 2000,)\n",
        "\n",
        "    # GET LABELS\n",
        "    Y_train=training_data['category'].values\n",
        "    Y_test=testing_data['category'].values\n",
        "     \n",
        "    # GET FEATURES\n",
        "    X_train,X_test,feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n",
        "\n",
        "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
        "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
        "    model=scikit_log_reg.fit(X_train,Y_train)\n",
        "\n",
        "    # GET TOP K PREDICTIONS\n",
        "    preds=get_top_k_predictions(model,X_test,top_k)\n",
        "    \n",
        "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
        "    eval_items=collect_preds(Y_test,preds)\n",
        "    \n",
        "    accuracy=compute_accuracy(eval_items)\n",
        "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
        "    \n",
        "    return model,feature_transformer,accuracy,mrr_at_k"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CwQFfHXXQvF",
        "outputId": "252c63a1-b826-40e1-9e49-fc32a108bb24"
      },
      "source": [
        "field='text_desc'\n",
        "feature_rep='binary'\n",
        "top_k=3\n",
        "\n",
        "model,transformer,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k)\n",
        "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]\n",
            "Accuracy=0.601990701329317; MRR=0.48291969528301365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dsq_C7VXXK8"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}